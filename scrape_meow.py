# Python 3.10+
import os, re, time, json
from pathlib import Path

import requests
import pandas as pd
from geopy.geocoders import ArcGIS
from geopy.extra.rate_limiter import RateLimiter

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ù–ê–°–¢–†–û–ô–ö–ò ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOKEN        = os.getenv("VK_TOKEN")                 # –¥–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ä–µ—Ç –≤ GitHub ‚Üí Settings ‚Üí Secrets ‚Üí Actions
DOMAIN       = os.getenv("VK_DOMAIN", "meowafisha")  # –ø–∞–±–ª–∏–∫ –í–ö
MAX_POSTS    = int(os.getenv("VK_MAX_POSTS", "2000"))
BATCH        = 100
WAIT_REQ     = 1.1                                   # VK ~1 rps
WAIT_GEO     = 1.0                                   # ArcGIS ‚âà1000/—Å—É—Ç–∫–∏
YEAR_DEFAULT = os.getenv("YEAR_DEFAULT", "2025")

OUTPUT_JSON  = Path("events.json")                   # —Ä–∞–∑–¥–∞—ë—Ç—Å—è Pages
CACHE_FILE   = Path("geocode_cache.json")            # –∫–æ–º–º–∏—Ç–∏–º –∫—ç—à ‚Äî —ç–∫–æ–Ω–æ–º–∏–º –ª–∏–º–∏—Ç

assert TOKEN, "VK_TOKEN –Ω–µ –∑–∞–¥–∞–Ω"

vk_url = "https://api.vk.com/method/wall.get"
geo = RateLimiter(ArcGIS(timeout=10).geocode, min_delay_seconds=WAIT_GEO)

# –ö—ç—à –∞–¥—Ä–µ—Å‚Üí[lat, lon]
if CACHE_FILE.exists():
    geocache = json.loads(CACHE_FILE.read_text(encoding="utf-8"))
else:
    geocache = {}

def vk_wall(offset: int):
    params = dict(domain=DOMAIN, offset=offset, count=BATCH,
                  access_token=TOKEN, v="5.199")
    r = requests.get(vk_url, params=params, timeout=20)
    r.raise_for_status()
    data = r.json()
    if "error" in data:
        raise RuntimeError(f"VK API error: {data['error']}")
    return data["response"]["items"]

CITY_WORDS = r"(–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥|–≥—É—Ä—å–µ–≤—Å–∫|—Å–≤–µ—Ç–ª–æ–≥–æ—Ä—Å–∫|—è–Ω—Ç–∞—Ä–Ω—ã–π|–±–∞–ª—Ç–∏–π—Å–∫|–ø–∏–æ–Ω–µ—Ä—Å–∫–∏–π|–∑–µ–ª–µ–Ω–æ–≥—Ä–∞–¥—Å–∫|–ø–æ—Å–µ–ª–æ–∫|–ø–æ—Å—ë–ª–æ–∫|–≥–æ—Ä–æ–¥|–ø–≥—Ç|–¥–µ—Ä–µ–≤–Ω—è|—Å–µ–ª–æ|—Å—Ç–∞–Ω—Ü–∏—è|—Å—Ç–∞–Ω—Ü–∏—è)"

def extract(text: str):
    m_date = re.search(r"\b(\d{2})\.(\d{2})\b", text)
    m_loc  = re.search(r"üìç\s*(.+)", text)
    if not (m_date and m_loc):
        return None

    date  = f"{YEAR_DEFAULT}-{m_date.group(2)}-{m_date.group(1)}"
    loc   = m_loc.group(1).split('‚û°Ô∏è')[0].strip()
    if not re.search(CITY_WORDS, loc, re.I):
        loc += ", –ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥"

    first_line = text.split('\n', 1)[0]
    title = re.sub(r"^\s*\d{2}\.\d{2}\s*\|\s*", "", first_line).strip()

    return dict(title=title, date=date, location=loc)

def geocode(addr: str):
    if addr in geocache:
        return geocache[addr]
    try:
        g = geo(addr)
        geocache[addr] = [g.latitude, g.longitude] if g else [None, None]
    except Exception:
        geocache[addr] = [None, None]
    return geocache[addr]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –°–ë–û–† –ü–û–°–¢–û–í ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
records, off = [], 0
while off < MAX_POSTS:
    items = vk_wall(off)
    if not items:
        break
    for it in items:
        text = it.get("text") or ""
        evt = extract(text)
        if evt:
            records.append(evt)
    off += BATCH
    time.sleep(WAIT_REQ)

print("–ê–Ω–æ–Ω—Å–æ–≤ –Ω–∞–π–¥–µ–Ω–æ:", len(records))
if not records:
    OUTPUT_JSON.write_text("[]", encoding="utf-8")
    raise SystemExit(0)

df = pd.DataFrame(records).drop_duplicates()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ì–ï–û–ö–û–î–ò–ù–ì ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
lats, lons = [], []
for addr in df["location"]:
    lat, lon = geocode(addr)
    lats.append(lat); lons.append(lon)
df["lat"] = lats; df["lon"] = lons

bad_cnt = int(df["lat"].isna().sum())
df = df.dropna(subset=["lat", "lon"])

print(f"–° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {len(df)} | –±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {bad_cnt}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –°–û–•–†–ê–ù–ï–ù–ò–ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df = df[["title","date","location","lat","lon"]].sort_values("date")
OUTPUT_JSON.write_text(df.to_json(orient="records", force_ascii=False, indent=2), encoding="utf-8")
CACHE_FILE.write_text(json.dumps(geocache, ensure_ascii=False, indent=2), encoding="utf-8")

print("‚úÖ  events.json —Å–æ–∑–¥–∞–Ω/–æ–±–Ω–æ–≤–ª—ë–Ω")
