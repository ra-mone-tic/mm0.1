name: meow

on:
  schedule:
    - cron: "0 */6 * * *"   # 00:00, 06:00, 12:00, 18:00 UTC
  workflow_dispatch:
    inputs:
      script_path:
        description: 'Path to the scraper script'
        required: false
        default: 'scrape_meow.py'

permissions:
  contents: read

concurrency:
  group: scrape-meow
  cancel-in-progress: false  # не убиваем предыдущий скрейп, если он ещё идёт

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install deps (if any)
        run: |
          if [ -f pyproject.toml ]; then
            pip install --upgrade pip
            pip install poetry
            poetry install
          elif [ -f requirements.txt ]; then
            pip install --upgrade pip
            pip install -r requirements.txt
          fi
          # прокинь секреты, если нужны (пример):
          # API_KEY: ${{ secrets.API_KEY }}
          # PROXY_URL: ${{ secrets.PROXY_URL }}
          # PYTHONUNBUFFERED=1 позволяет сразу видеть вывод логов (без буферизации)
          PYTHONUNBUFFERED: "1"
          # прокинь секреты, если нужны (пример):
          # API_KEY: ${{ secrets.API_KEY }}
          # PROXY_URL: ${{ secrets.PROXY_URL }}
        run: |
          python "${{ github.event.inputs.script_path }}"
          # если файл лежит в другом месте, укажи путь при запуске workflow
          # python scripts/scrape_meow.py

      # пример: сохранить артефакт (логи/результаты) — опционально
      # - name: Upload results
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: scrape-output
      #     path: output/**
