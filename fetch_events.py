#!/usr/bin/env python3
"""
MeowAfisha ¬∑ fetch_events.py
–£–ª—É—á—à–µ–Ω–æ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
- –ö–∞—Å–∫–∞–¥–Ω—ã–π –≥–µ–æ–∫–æ–¥–∏–Ω–≥: ArcGIS ‚Üí Yandex ‚Üí Nominatim
- –ö—ç—à: geocode_cache.json (–∫–æ–º–º–∏—Ç–∏–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –∫–≤–æ—Ç API)
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ: stdout + –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ geocode_log.json —Å GEOCODE_SAVE_LOG=1
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏: min_delay_seconds –Ω–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (env)
"""

import os
import re
import time
import json
import sys
import logging
from pathlib import Path

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ .env –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import pandas as pd
from geopy.geocoders import ArcGIS, Yandex, Nominatim
from geopy.extra.rate_limiter import RateLimiter
import geopy.exc

# Configure logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
if not logger.handlers:
    logger.addHandler(handler)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ù–ê–°–¢–†–û–ô–ö–ê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOKEN = os.getenv("VK_TOKEN")  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π —Å–µ–∫—Ä–µ—Ç VK
DOMAIN = os.getenv("VK_DOMAIN", "meowafisha")
MAX_POSTS = int(os.getenv("VK_MAX_POSTS", "50"))
BATCH = 100
WAIT_REQ = float(os.getenv("VK_WAIT_REQ", "1.1"))  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É wall.get (~1 rps)
YEAR_DEFAULT = os.getenv("YEAR_DEFAULT", "2025")

# –ó–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≥–µ–æ–∫–æ–¥–∏–Ω–≥–∞ (—Å–µ–∫—É–Ω–¥—ã)
DEFAULT_DELAYS = {
    'ARCGIS': float(os.getenv("ARCGIS_MIN_DELAY", "1.0")),
    'YANDEX': float(os.getenv("YANDEX_MIN_DELAY", "1.0")),
    'NOMINATIM': float(os.getenv("NOMINATIM_MIN_DELAY", "1.0"))
}

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –ª–æ–≥–∞ –≤ —Ñ–∞–π–ª
GEOCODE_SAVE_LOG = os.getenv("GEOCODE_SAVE_LOG", "1") == "1"

OUTPUT_JSON = Path("events.json")
CACHE_FILE = Path("geocode_cache.json")
LOG_FILE = Path("geocode_log.json")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –£–¢–ò–õ–ò–¢–´ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def init_session() -> requests.Session:
    """–°–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é requests —Å –ª–æ–≥–∏–∫–æ–π –ø–æ–≤—Ç–æ—Ä–∞."""
    session = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"],
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session

session = init_session()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–æ–∫–æ–¥–µ—Ä–æ–≤
arcgis = ArcGIS(timeout=10)
yandex = Yandex(api_key=os.getenv("YANDEX_KEY"), timeout=10, user_agent="meowafisha-script") if os.getenv("YANDEX_KEY") else None
nominatim_url = os.getenv("NOMINATIM_URL", "").strip()
if nominatim_url:
    nominatim = Nominatim(user_agent=os.getenv("NOMINATIM_USER_AGENT", "meowafisha-bot"), timeout=10, domain=nominatim_url)
else:
    nominatim = Nominatim(user_agent=os.getenv("NOMINATIM_USER_AGENT", "meowafisha-bot"), timeout=10)

# –û–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ (–æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–µ 1 –∑–∞–ø—Ä–æ—Å/—Å –Ω–∞ —Å–µ—Ä–≤–∏—Å)
arcgis_geocode = RateLimiter(arcgis.geocode, min_delay_seconds=DEFAULT_DELAYS['ARCGIS']) if arcgis else None
yandex_geocode = RateLimiter(yandex.geocode, min_delay_seconds=DEFAULT_DELAYS['YANDEX']) if yandex else None
nominatim_geocode = RateLimiter(nominatim.geocode, min_delay_seconds=DEFAULT_DELAYS['NOMINATIM']) if nominatim else None

GEOCODERS = [
    {"name": "ArcGIS", "func": arcgis_geocode},
    {"name": "Yandex", "func": yandex_geocode},
    {"name": "Nominatim", "func": nominatim_geocode},
]

# –í—Ä–µ–º–µ–Ω–Ω—ã–π –ª–æ–≥ –≥–µ–æ–∫–æ–¥–∏–Ω–≥–∞ (–∞–¥—Ä–µ—Å ‚Üí {'arcgis':..., 'yandex':..., 'nominatim':...})
geolog = {}
geocache = {}
original_cache = {}

def log_geocoding(addr: str, provider: str, success: bool, detail: str = ""):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏."""
    msg = f"[{provider:9}] {'OK ' if success else 'N/A'} | {addr}"
    if detail:
        msg += f" ‚Üí {detail}"

    level = logging.INFO if success else logging.WARNING
    logger.log(level, msg)

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ geolog –¥–ª—è JSON —ç–∫—Å–ø–æ—Ä—Ç–∞
    if addr not in geolog:
        geolog[addr] = {}
    geolog[addr][provider] = {"success": success, "detail": detail}

def load_cache() -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à –≥–µ–æ–∫–æ–¥–∏–Ω–≥–∞ –∏–∑ —Ñ–∞–π–ª–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
    if not CACHE_FILE.exists():
        logger.info("–§–∞–π–ª –∫—ç—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å —á–∏—Å—Ç–æ–≥–æ")
        return {}

    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            cache = json.load(f)
        logger.info(f"–ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω: {len(cache)} –∞–¥—Ä–µ—Å–æ–≤")
        return cache
    except (json.JSONDecodeError, IOError) as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à: {e}, –Ω–∞—á–∏–Ω–∞–µ–º —Å —á–∏—Å—Ç–æ–≥–æ")
        return {}

def save_cache(cache: dict, force: bool = False) -> None:
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –≥–µ–æ–∫–æ–¥–∏–Ω–≥–∞ –Ω–∞ –¥–∏—Å–∫."""
    if cache == original_cache and not force:
        logger.info("–ö—ç—à –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
        return

    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        logger.info(f"–ö—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(cache)} –∞–¥—Ä–µ—Å–æ–≤")
    except IOError as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")

def geocode_addr(addr: str) -> tuple:
    """–ö–∞—Å–∫–∞–¥–Ω—ã–π –≥–µ–æ–∫–æ–¥–∏–Ω–≥ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
    if not addr or not addr.strip():
        logger.warning("–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—É—Å—Ç–æ–π –∞–¥—Ä–µ—Å")
        return (None, None)

    addr = addr.strip()

    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à
    if addr in geocache:
        cached_coords = geocache[addr]
        if cached_coords != [None, None]:
            logger.info(f"[CACHE    ] HIT | {addr} ‚Üí {cached_coords[0]:.6f},{cached_coords[1]:.6f}")
            return tuple(cached_coords)
        else:
            logger.info(f"[CACHE    ] HIT | {addr} ‚Üí –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ—Ä–≤–∏—Å—ã –≥–µ–æ–∫–æ–¥–∏–Ω–≥–∞
    for provider in GEOCODERS:
        name, func = provider["name"], provider["func"]
        if not func:
            log_geocoding(addr, name, False, "key not configured")
            continue

        try:
            loc = func(addr)
            if loc:
                coords = [loc.latitude, loc.longitude]
                geocache[addr] = coords
                log_geocoding(addr, name, True, f"{coords[0]:.6f},{coords[1]:.6f}")
                return tuple(coords)
            else:
                log_geocoding(addr, name, False, "no result")
        except requests.exceptions.RequestException as e:
            log_geocoding(addr, name, False, f"HTTP error: {e}")
        except geopy.exc.GeopyError as e:
            log_geocoding(addr, name, False, f"Geocoding error: {e}")
        except Exception as e:
            log_geocoding(addr, name, False, f"Unexpected error: {e}")

    # –í—Å–µ –≥–µ–æ–∫–æ–¥–µ—Ä—ã –Ω–µ —É–¥–∞–ª–∏—Å—å
    geocache[addr] = [None, None]
    logger.warning(f"–í—Å–µ –≥–µ–æ–∫–æ–¥–µ—Ä—ã –Ω–µ —É–¥–∞–ª–∏—Å—å –¥–ª—è: {addr}")
    return (None, None)

def vk_wall(offset: int, attempts: int = 3):
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å—Ç—ã —Å—Ç–µ–Ω—ã VK —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏."""
    params = {
        'domain': DOMAIN,
        'offset': offset,
        'count': BATCH,
        'access_token': TOKEN,
        'v': '5.199'
    }

    for attempt in range(1, attempts + 1):
        try:
            r = session.get("https://api.vk.ru/method/wall.get", params=params, timeout=20)
            r.raise_for_status()

            data = r.json()
            if 'error' in data:
                raise RuntimeError(f"VK API error: {data['error']}")

            return data['response']['items']

        except requests.exceptions.Timeout:
            logger.warning(f"VK request timeout (attempt {attempt}/{attempts})")
        except requests.exceptions.RequestException as e:
            logger.error(f"VK request failed (attempt {attempt}/{attempts}): {e}")
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON from VK (attempt {attempt}/{attempts}): {e}")
        except KeyError as e:
            logger.error(f"Unexpected VK response format (attempt {attempt}/{attempts}): {e}")

        if attempt < attempts:
            sleep_time = WAIT_REQ * attempt  # progressive backoff
            logger.info(f"Retrying in {sleep_time:.1f}s...")
            time.sleep(sleep_time)

    raise RuntimeError(f"Failed to fetch VK data after {attempts} attempts")

CITY_WORDS = r"(–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥|–≥—É—Ä—å–µ–≤—Å–∫|—Å–≤–µ—Ç–ª–æ–≥–æ—Ä—Å–∫|—è–Ω—Ç–∞—Ä–Ω—ã–π|–∑–µ–ª–µ–Ω–æ–≥—Ä–∞–¥—Å–∫|–ø–∏–æ–Ω–µ—Ä—Å–∫–∏–π|–±–∞–ª—Ç–∏–π—Å–∫|–ø–æ—Å–µ–ª–æ–∫|–ø–æ—Å\.|–≥\.)"

def extract(text: str):
    """–ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞ VK."""
    if not text:
        return None

    # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–∞—Ç—ã
    date_patterns = [
        r"\b(\d{2})\.(\d{2})\b",  # DD.MM
        r"\b(\d{2})/(\d{2})\b",   # DD/MM
        r"\b(\d{1,2})\.(\d{1,2})\b",  # D.M –∏–ª–∏ DD.MM
    ]

    date_match = None
    for pattern in date_patterns:
        date_match = re.search(pattern, text)
        if date_match:
            break

    # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è
    loc_patterns = [
        r"üìç\s*(.+)",      # üìç
        r"üìç\s*([^üìç\n]+)", # üìç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç–º–æ–¥–∑–∏ –∏–ª–∏ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
        r"–º–µ—Å—Ç–æ[:\s]*(.+)", # "–º–µ—Å—Ç–æ:"
        r"–∞–¥—Ä–µ—Å[:\s]*(.+)", # "–∞–¥—Ä–µ—Å:"
    ]

    loc_match = None
    for pattern in loc_patterns:
        loc_match = re.search(pattern, text, re.I)
        if loc_match:
            break

    if not (date_match and loc_match):
        logger.debug(f"No date or location found in post: {text[:100]}...")
        return None

    date = f"{YEAR_DEFAULT}-{date_match.group(2).zfill(2)}-{date_match.group(1).zfill(2)}"
    loc = loc_match.group(1).split('‚û°Ô∏è')[0].split('\n')[0].strip()

    # –î–æ–±–∞–≤–∏—Ç—å –≥–æ—Ä–æ–¥ –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    if not re.search(CITY_WORDS, loc, re.I):
        loc += ", –ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥"

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫: —Å—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π –±–µ–∑ "DD.MM |"
    lines = text.split('\n')
    title = ""
    for line in lines:
        if re.search(r"\b\d{1,2}[./]\d{1,2}\b", line):
            title = re.sub(r"^\s*\d{1,2}[./]\d{1,2}\s*\|\s*", "", line).strip()
            break

    # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—É—Å—Ç–æ–π, –≤–∑—è—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
    if not title:
        title = lines[0].strip() if lines else "–°–æ–±—ã—Ç–∏–µ"

    return {
        'title': title,
        'date': date,
        'location': loc,
        'text': text
    }

def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
    logger.info(f"VK_TOKEN present: {bool(TOKEN)}")
    logger.info(f"DOMAIN: {DOMAIN}")
    if not TOKEN:
        logger.critical("VK_TOKEN –Ω–µ –∑–∞–¥–∞–Ω (—Å–µ–∫—Ä–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏–ª–∏ .env —Ç—Ä–µ–±—É–µ—Ç—Å—è)")
        sys.exit(1)

    try:
        logger.info("–ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π...")

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–æ–±—ã—Ç–∏—è
        existing_events = []
        if OUTPUT_JSON.exists():
            try:
                existing_events = json.loads(OUTPUT_JSON.read_text(encoding='utf-8'))
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_events)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–æ–±—ã—Ç–∏–π")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–æ–±—ã—Ç–∏—è: {e}")

        # –°–æ–∑–¥–∞—Ç—å set –∫–ª—é—á–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        existing_keys = set()
        for event in existing_events:
            key = f"{event['date']}|{event['title']}|{event['location']}"
            existing_keys.add(key)

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à
        global geocache, original_cache, geolog
        geocache = load_cache()
        original_cache = geocache.copy()
        geolog = {}

        # –°–æ–±—Ä–∞—Ç—å –ø–æ—Å—Ç—ã
        records, offset = [], 0
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ {MAX_POSTS} –ø–æ—Å—Ç–æ–≤ –∏–∑ –≥—Ä—É–ø–ø—ã VK '{DOMAIN}'")

        while offset < MAX_POSTS:
            try:
                items = vk_wall(offset)
                if not items:
                    logger.info("–ë–æ–ª—å—à–µ –ø–æ—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    break

                for item in items:
                    text = item.get("text") or ""
                    logger.debug(f"Processing post: {text[:200]}...")
                    event = extract(text)
                    if event:
                        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–æ–≤–æ–µ –ª–∏ —Å–æ–±—ã—Ç–∏–µ
                        event_key = f"{event['date']}|{event['title']}|{event['location']}"
                        if event_key not in existing_keys:
                            records.append(event)
                            existing_keys.add(event_key)  # –î–æ–±–∞–≤–∏—Ç—å –≤ set —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                        else:
                            logger.debug(f"–°–æ–±—ã—Ç–∏–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {event['title']}")

                offset += BATCH

                if offset < MAX_POSTS:
                    time.sleep(WAIT_REQ)

            except Exception as e:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–∞—Ç—á —Å —Å–º–µ—â–µ–Ω–∏–µ–º {offset}: {e}")
                break

        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(records)} —Å–æ–±—ã—Ç–∏–π")

        if not records:
            logger.warning("–ù–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ")
            # –ù–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª, –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–æ–±—ã—Ç–∏—è
            return

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        df = pd.DataFrame(records).drop_duplicates()
        logger.info(f"–ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π")

        # –ì–µ–æ–∫–æ–¥–∏–Ω–≥ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        lats, lons = [], []
        processed = 0

        for addr in df["location"]:
            if processed % 10 == 0:
                logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å –≥–µ–æ–∫–æ–¥–∏–Ω–≥–∞: {processed}/{len(df)}")

            lat, lon = geocode_addr(addr)
            lats.append(lat)
            lons.append(lon)
            processed += 1

        df["lat"] = lats
        df["lon"] = lons

        # –°–æ–æ–±—â–∏—Ç—å –æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö
        missing = df[df["lat"].isna()]
        missing_count = len(missing)
        if missing_count > 0:
            missing_addrs = ", ".join(sorted(set(missing["location"].tolist())))
            logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è {missing_count} –∞–¥—Ä–µ—Å–æ–≤: {missing_addrs[:800]}{'...' if len(missing_addrs) > 800 else ''}")

        # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Å–æ–±—ã—Ç–∏—è –±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        df = df.dropna(subset=["lat", "lon"])
        logger.info(f"–§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(df)} –Ω–æ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏")

        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏
        new_events = df[["title", "date", "location", "lat", "lon", "text"]].to_dict('records')
        all_events = existing_events + new_events

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–∞—Ç–µ
        all_events.sort(key=lambda x: x['date'])

        logger.info(f"–û–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {len(all_events)} —Å–æ–±—ã—Ç–∏–π ({len(existing_events)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö + {len(new_events)} –Ω–æ–≤—ã—Ö)")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        OUTPUT_JSON.write_text(
            json.dumps(all_events, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à
        save_cache(geocache)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if GEOCODE_SAVE_LOG:
            try:
                LOG_FILE.write_text(json.dumps(geolog, ensure_ascii=False, indent=2), encoding="utf-8")
            except Exception as e:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–≥ –≥–µ–æ–∫–æ–¥–∏–Ω–≥–∞: {e}")

        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    except Exception as e:
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main: {e}", exc_info=True)
        sys.exit(1)
    finally:
        # –í—Å–µ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞—Ç—å —Å–µ—Å—Å–∏—é
        session.close()
        logger.info("–°–µ—Å—Å–∏—è –∑–∞–∫—Ä—ã—Ç–∞")

if __name__ == "__main__":
    main()
